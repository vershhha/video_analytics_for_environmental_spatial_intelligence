{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "537035af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Augmenters that apply affine transformations.\n",
    "\n",
    "To use the augmenters, clone the complete repo and use\n",
    "`from vidaug import augmenters as va`\n",
    "and then e.g. :\n",
    "    seq = va.Sequential([ va.RandomRotate(30),\n",
    "                          va.RandomResize(0.2)  ])\n",
    "\n",
    "List of augmenters:\n",
    "    * RandomRotate\n",
    "    * RandomResize\n",
    "    * RandomTranslate\n",
    "    * RandomShear\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import numbers\n",
    "import random\n",
    "import scipy\n",
    "import skimage\n",
    "import PIL\n",
    "import cv2\n",
    "\n",
    "\n",
    "class RandomRotate(object):\n",
    "    \"\"\"\n",
    "    Rotate video randomly by a random angle within given boundsi.\n",
    "\n",
    "    Args:\n",
    "        degrees (sequence or int): Range of degrees to randomly\n",
    "        select from. If degrees is a number instead of sequence\n",
    "        like (min, max), the range of degrees, will be\n",
    "        (-degrees, +degrees).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degrees):\n",
    "        if isinstance(degrees, numbers.Number):\n",
    "            if degrees < 0:\n",
    "                raise ValueError('If degrees is a single number,'\n",
    "                                 'must be positive')\n",
    "            degrees = (-degrees, degrees)\n",
    "        else:\n",
    "            if len(degrees) != 2:\n",
    "                raise ValueError('If degrees is a sequence,'\n",
    "                                 'it must be of len 2.')\n",
    "\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        angle = random.uniform(self.degrees[0], self.degrees[1])\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            rotated = [skimage.transform.rotate(img, angle) for img in clip]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            rotated = [img.rotate(angle) for img in clip]\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "\n",
    "        return rotated\n",
    "\n",
    "\n",
    "class RandomResize(object):\n",
    "    \"\"\"\n",
    "    Resize video bysoomingin and out.\n",
    "\n",
    "    Args:\n",
    "        rate (float): Video is scaled uniformly between\n",
    "        [1 - rate, 1 + rate].\n",
    "\n",
    "        interp (string): Interpolation to use for re-sizing\n",
    "        ('nearest', 'lanczos', 'bilinear', 'bicubic' or 'cubic').\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rate=0.0, interp='bilinear'):\n",
    "        self.rate = rate\n",
    "\n",
    "        self.interpolation = interp\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        scaling_factor = random.uniform(1 - self.rate, 1 + self.rate)\n",
    "\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            im_h, im_w, im_c = clip[0].shape\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            im_w, im_h = clip[0].size\n",
    "\n",
    "        new_w = int(im_w * scaling_factor)\n",
    "        new_h = int(im_h * scaling_factor)\n",
    "        new_size = (new_h, new_w)\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            return [scipy.misc.imresize(img, size=(new_h, new_w),interp=self.interpolation) for img in clip]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            return [img.resize(size=(new_w, new_h), resample=self._get_PIL_interp(self.interpolation)) for img in clip]\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "\n",
    "    def _get_PIL_interp(self, interp):\n",
    "        if interp == 'nearest':\n",
    "            return PIL.Image.NEAREST\n",
    "        elif interp == 'lanczos':\n",
    "            return PIL.Image.LANCZOS\n",
    "        elif interp == 'bilinear':\n",
    "            return PIL.Image.BILINEAR\n",
    "        elif interp == 'bicubic':\n",
    "            return PIL.Image.BICUBIC\n",
    "        elif interp == 'cubic':\n",
    "            return PIL.Image.CUBIC\n",
    "\n",
    "\n",
    "class RandomTranslate(object):\n",
    "    \"\"\"\n",
    "      Shifting video in X and Y coordinates.\n",
    "\n",
    "        Args:\n",
    "            x (int) : Translate in x direction, selected\n",
    "            randomly from [-x, +x] pixels.\n",
    "\n",
    "            y (int) : Translate in y direction, selected\n",
    "            randomly from [-y, +y] pixels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x=0, y=0):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        x_move = random.randint(-self.x, +self.x)\n",
    "        y_move = random.randint(-self.y, +self.y)\n",
    "\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            rows, cols, ch = clip[0].shape\n",
    "            transform_mat = np.float32([[1, 0, x_move], [0, 1, y_move]])\n",
    "            return [cv2.warpAffine(img, transform_mat, (cols, rows)) for img in clip]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            return [img.transform(img.size, PIL.Image.AFFINE, (1, 0, x_move, 0, 1, y_move)) for img in clip]\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "\n",
    "\n",
    "class RandomShear(object):\n",
    "    \"\"\"\n",
    "    Shearing video in X and Y directions.\n",
    "\n",
    "    Args:\n",
    "        x (int) : Shear in x direction, selected randomly from\n",
    "        [-x, +x].\n",
    "\n",
    "        y (int) : Shear in y direction, selected randomly from\n",
    "        [-y, +y].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        x_shear = random.uniform(-self.x, self.x)\n",
    "        y_shear = random.uniform(-self.y, self.y)\n",
    "\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            rows, cols, ch = clip[0].shape\n",
    "            transform_mat = np.float32([[1, x_shear, 0], [y_shear, 1, 0]])\n",
    "            return [cv2.warpAffine(img, transform_mat, (cols, rows)) for img in clip]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            return [img.transform(img.size, PIL.Image.AFFINE, (1, x_shear, 0, y_shear, 1, 0)) for img in clip]\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                                'but got list of {0}'.format(type(clip[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5de11304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Augmenters that apply video flipping horizontally and\n",
    "vertically.\n",
    "\n",
    "To use the augmenters, clone the complete repo and use\n",
    "`from vidaug import augmenters as va`\n",
    "and then e.g. :\n",
    "    seq = va.Sequential([ va.HorizontalFlip(),\n",
    "                          va.VerticalFlip() ])\n",
    "\n",
    "List of augmenters:\n",
    "    * HorizontalFlip\n",
    "    * VerticalFlip\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "\n",
    "class HorizontalFlip(object):\n",
    "    \"\"\"\n",
    "    Horizontally flip the video.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            return [np.fliplr(img) for img in clip]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            return [img.transpose(PIL.Image.FLIP_LEFT_RIGHT) for img in clip]\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            ' but got list of {0}'.format(type(clip[0])))\n",
    "\n",
    "\n",
    "\n",
    "class VerticalFlip(object):\n",
    "    \"\"\"\n",
    "    Vertically flip the video.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            return [np.flipud(img) for img in clip]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            return [img.transpose(PIL.Image.FLIP_TOP_BOTTOM) for img in clip]\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            ' but got list of {0}'.format(type(clip[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "612b7882",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Augmenters that apply video flipping horizontally and\n",
    "vertically.\n",
    "\n",
    "To use the augmenters, clone the complete repo and use\n",
    "`from vidaug import augmenters as va`\n",
    "and then e.g. :\n",
    "    seq = va.Sequential([ va.HorizontalFlip(),\n",
    "                          va.VerticalFlip() ])\n",
    "\n",
    "List of augmenters:\n",
    "    * CenterCrop\n",
    "    * CornerCrop\n",
    "    * RandomCrop\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import numbers\n",
    "import random\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    \"\"\"\n",
    "    Extract center crop of thevideo.\n",
    "\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size for the crop in format (h, w).\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            if size < 0:\n",
    "                raise ValueError('If size is a single number, it must be positive')\n",
    "            size = (size, size)\n",
    "        else:\n",
    "            if len(size) != 2:\n",
    "                raise ValueError('If size is a sequence, it must be of len 2.')\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        crop_h, crop_w = self.size\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            im_h, im_w, im_c = clip[0].shape\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            im_w, im_h = clip[0].size\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "\n",
    "        if crop_w > im_w or crop_h > im_h:\n",
    "            error_msg = ('Initial image size should be larger then' +\n",
    "                         'cropped size but got cropped sizes : ' +\n",
    "                         '({w}, {h}) while initial image is ({im_w}, ' +\n",
    "                         '{im_h})'.format(im_w=im_w, im_h=im_h, w=crop_w,\n",
    "                                          h=crop_h))\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        w1 = int(round((im_w - crop_w) / 2.))\n",
    "        h1 = int(round((im_h - crop_h) / 2.))\n",
    "\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            return [img[h1:h1 + crop_h, w1:w1 + crop_w, :] for img in clip]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            return [img.crop((w1, h1, w1 + crop_w, h1 + crop_h)) for img in clip]\n",
    "\n",
    "\n",
    "class CornerCrop(object):\n",
    "    \"\"\"\n",
    "    Extract corner crop of the video.\n",
    "\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size for the crop in format (h, w).\n",
    "\n",
    "        crop_position (str): Selected corner (or center) position from the\n",
    "        list ['c', 'tl', 'tr', 'bl', 'br']. If it is non, crop position is\n",
    "        selected randomly at each call.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, crop_position=None):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            if size < 0:\n",
    "                raise ValueError('If size is a single number, it must be positive')\n",
    "            size = (size, size)\n",
    "        else:\n",
    "            if len(size) != 2:\n",
    "                raise ValueError('If size is a sequence, it must be of len 2.')\n",
    "        self.size = size\n",
    "\n",
    "        if crop_position is None:\n",
    "            self.randomize = True\n",
    "        else:\n",
    "            if crop_position not in ['c', 'tl', 'tr', 'bl', 'br']:\n",
    "                raise ValueError(\"crop_position should be one of \" +\n",
    "                                 \"['c', 'tl', 'tr', 'bl', 'br']\")\n",
    "            self.randomize = False\n",
    "        self.crop_position = crop_position\n",
    "        self.crop_positions = ['c', 'tl', 'tr', 'bl', 'br']\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        crop_h, crop_w = self.size\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            im_h, im_w, im_c = clip[0].shape\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            im_w, im_h = clip[0].size\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "\n",
    "        if self.randomize:\n",
    "            self.crop_position = self.crop_positions[random.randint(0,len(self.crop_positions) - 1)]\n",
    "\n",
    "        if self.crop_position == 'c':\n",
    "            th, tw = (self.size, self.size)\n",
    "            x1 = int(round((im_w - crop_w) / 2.))\n",
    "            y1 = int(round((im_h - crop_h) / 2.))\n",
    "            x2 = x1 + crop_w\n",
    "            y2 = y1 + crop_h\n",
    "        elif self.crop_position == 'tl':\n",
    "            x1 = 0\n",
    "            y1 = 0\n",
    "            x2 = crop_w\n",
    "            y2 = crop_h\n",
    "        elif self.crop_position == 'tr':\n",
    "            x1 = im_w - crop_w\n",
    "            y1 = 0\n",
    "            x2 = im_w\n",
    "            y2 = crop_h\n",
    "        elif self.crop_position == 'bl':\n",
    "            x1 = 0\n",
    "            y1 = im_h - crop_h\n",
    "            x2 = crop_w\n",
    "            y2 = im_h\n",
    "        elif self.crop_position == 'br':\n",
    "            x1 = im_w - crop_w\n",
    "            y1 = im_h - crop_h\n",
    "            x2 = im_w\n",
    "            y2 = im_h\n",
    "\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            return [img[y1:y2, x1:x2, :] for img in clip]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            return [img.crop((x1, y1, x2, y2)) for img in clip]\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"\n",
    "    Extract random crop of the video.\n",
    "\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size for the crop in format (h, w).\n",
    "\n",
    "        crop_position (str): Selected corner (or center) position from the\n",
    "        list ['c', 'tl', 'tr', 'bl', 'br']. If it is non, crop position is\n",
    "        selected randomly at each call.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            if size < 0:\n",
    "                raise ValueError('If size is a single number, it must be positive')\n",
    "            size = (size, size)\n",
    "        else:\n",
    "            if len(size) != 2:\n",
    "                raise ValueError('If size is a sequence, it must be of len 2.')\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        crop_h, crop_w = self.size\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            im_h, im_w, im_c = clip[0].shape\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            im_w, im_h = clip[0].size\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "\n",
    "        if crop_w > im_w or crop_h > im_h:\n",
    "            error_msg = ('Initial image size should be larger then' +\n",
    "                         'cropped size but got cropped sizes : ' +\n",
    "                         '({w}, {h}) while initial image is ({im_w}, ' +\n",
    "                         '{im_h})'.format(im_w=im_w, im_h=im_h, w=crop_w,\n",
    "                                          h=crop_h))\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        w1 = random.randint(0, im_w - crop_w)\n",
    "        h1 = random.randint(0, im_h - crop_h)\n",
    "\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            return [img[h1:h1 + crop_h, w1:w1 + crop_w, :] for img in clip]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            return [img.crop((w1, h1, w1 + crop_w, h1 + crop_h)) for img in clip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e630c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Augmenters that apply geometric transformations.\n",
    "\n",
    "To use the augmenters, clone the complete repo and use\n",
    "`from vidaug import augmenters as va`\n",
    "and then e.g. :\n",
    "    seq = va.Sequential([ va.RandomRotate(30),\n",
    "                          va.RandomResize(0.2)  ])\n",
    "\n",
    "List of augmenters:\n",
    "    * GaussianBlur\n",
    "    * ElasticTransformation\n",
    "    * PiecewiseAffineTransform\n",
    "    * Superpixel\n",
    "\"\"\"\n",
    "\n",
    "from skimage import segmentation, measure\n",
    "import numpy as np\n",
    "import random\n",
    "import numbers\n",
    "import scipy\n",
    "import PIL\n",
    "import cv2\n",
    "\n",
    "\n",
    "class GaussianBlur(object):\n",
    "    \"\"\"\n",
    "    Augmenter to blur images using gaussian kernels.\n",
    "\n",
    "    Args:\n",
    "        sigma (float): Standard deviation of the gaussian kernel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, clip):\n",
    "\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            return [scipy.ndimage.gaussian_filter(img, sigma=self.sigma, order=0) for img in clip]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            return [img.filter(PIL.ImageFilter.GaussianBlur(radius=self.sigma)) for img in clip]\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "\n",
    "\n",
    "class ElasticTransformation(object):\n",
    "    \"\"\"\n",
    "    Augmenter to transform images by moving pixels locally around using\n",
    "    displacement fields.\n",
    "    See\n",
    "        Simard, Steinkraus and Platt\n",
    "        Best Practices for Convolutional Neural Networks applied to Visual\n",
    "        Document Analysis\n",
    "        in Proc. of the International Conference on Document Analysis and\n",
    "        Recognition, 2003\n",
    "    for a detailed explanation.\n",
    "\n",
    "    Args:\n",
    "        alpha (float): Strength of the distortion field. Higher values mean\n",
    "        more \"movement\" of pixels.\n",
    "\n",
    "        sigma (float): Standard deviation of the gaussian kernel used to\n",
    "        smooth the distortion fields.\n",
    "\n",
    "        order (int): Interpolation order to use. Same meaning as in\n",
    "        `scipy.ndimage.map_coordinates` and may take any integer value in\n",
    "        the range 0 to 5, where orders close to 0 are faster.\n",
    "\n",
    "        cval (int): The constant intensity value used to fill in new pixels.\n",
    "        This value is only used if `mode` is set to \"constant\".\n",
    "        For standard uint8 images (value range 0-255), this value may also\n",
    "        come from the range 0-255. It may be a float value, even for\n",
    "        integer image dtypes.\n",
    "\n",
    "        mode : Parameter that defines the handling of newly created pixels.\n",
    "        May take the same values as in `scipy.ndimage.map_coordinates`,\n",
    "        i.e. \"constant\", \"nearest\", \"reflect\" or \"wrap\".\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0, sigma=0, order=3, cval=0, mode=\"constant\",\n",
    "                 name=None, deterministic=False):\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "        self.order = order\n",
    "        self.cval = cval\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, clip):\n",
    "\n",
    "        is_PIL = isinstance(clip[0], PIL.Image.Image)\n",
    "        if is_PIL:\n",
    "            clip = [np.asarray(img) for img in clip]\n",
    "\n",
    "        result = []\n",
    "        nb_images = len(clip)\n",
    "        for i in range(nb_images):\n",
    "            image = clip[i]\n",
    "            image_first_channel = np.squeeze(image[..., 0])\n",
    "            indices_x, indices_y = self._generate_indices(image_first_channel.shape, alpha=self.alpha, sigma=self.sigma)\n",
    "            result.append(self._map_coordinates(\n",
    "                clip[i],\n",
    "                indices_x,\n",
    "                indices_y,\n",
    "                order=self.order,\n",
    "                cval=self.cval,\n",
    "                mode=self.mode))\n",
    "\n",
    "        if is_PIL:\n",
    "            return [PIL.Image.fromarray(img) for img in result]\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def _generate_indices(self, shape, alpha, sigma):\n",
    "        assert (len(shape) == 2),\"shape: Should be of size 2!\"\n",
    "        dx = scipy.ndimage.gaussian_filter((np.random.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "        dy = scipy.ndimage.gaussian_filter((np.random.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "\n",
    "        x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n",
    "        return np.reshape(x+dx, (-1, 1)), np.reshape(y+dy, (-1, 1))\n",
    "\n",
    "    def _map_coordinates(self, image, indices_x, indices_y, order=1, cval=0, mode=\"constant\"):\n",
    "        assert (len(image.shape) == 3),\"image.shape: Should be of size 3!\"\n",
    "        result = np.copy(image)\n",
    "        height, width = image.shape[0:2]\n",
    "        for c in range(image.shape[2]):\n",
    "            remapped_flat = scipy.ndimage.interpolation.map_coordinates(\n",
    "                image[..., c],\n",
    "                (indices_x, indices_y),\n",
    "                order=order,\n",
    "                cval=cval,\n",
    "                mode=mode\n",
    "            )\n",
    "            remapped = remapped_flat.reshape((height, width))\n",
    "            result[..., c] = remapped\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "class PiecewiseAffineTransform(object):\n",
    "    \"\"\"\n",
    "    Augmenter that places a regular grid of points on an image and randomly\n",
    "    moves the neighbourhood of these point around via affine transformations.\n",
    "\n",
    "     Args:\n",
    "         displacement (init): gives distorted image depending on the valuse of displacement_magnification and displacement_kernel\n",
    "\n",
    "         displacement_kernel (init): gives the blury effect\n",
    "\n",
    "         displacement_magnification (float): it magnify the image\n",
    "    \"\"\"\n",
    "    def __init__(self, displacement=0, displacement_kernel=0, displacement_magnification=0):\n",
    "        self.displacement = displacement\n",
    "        self.displacement_kernel = displacement_kernel\n",
    "        self.displacement_magnification = displacement_magnification\n",
    "\n",
    "    def __call__(self, clip):\n",
    "\n",
    "        ret_img_group = clip\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            im_size = clip[0].shape\n",
    "            image_w, image_h = im_size[1], im_size[0]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            im_size = clip[0].size\n",
    "            image_w, image_h = im_size[0], im_size[1]\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "\n",
    "        displacement_map = np.random.rand(image_h, image_w, 2) * 2 * self.displacement - self.displacement\n",
    "        displacement_map = cv2.GaussianBlur(displacement_map, None,\n",
    "                                            self.displacement_kernel)\n",
    "        displacement_map *= self.displacement_magnification * self.displacement_kernel\n",
    "        displacement_map = np.floor(displacement_map).astype('int32')\n",
    "\n",
    "        displacement_map_rows = displacement_map[..., 0] + np.tile(np.arange(image_h), (image_w, 1)).T.astype('int32')\n",
    "        displacement_map_rows = np.clip(displacement_map_rows, 0, image_h - 1)\n",
    "\n",
    "        displacement_map_cols = displacement_map[..., 1] + np.tile(np.arange(image_w), (image_h, 1)).astype('int32')\n",
    "        displacement_map_cols = np.clip(displacement_map_cols, 0, image_w - 1)\n",
    "\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            return [img[(displacement_map_rows.flatten(), displacement_map_cols.flatten())].reshape(img.shape) for img in clip]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            return [PIL.Image.fromarray(np.asarray(img)[(displacement_map_rows.flatten(), displacement_map_cols.flatten())].reshape(np.asarray(img).shape)) for img in clip]\n",
    "\n",
    "\n",
    "\n",
    "class Superpixel(object):\n",
    "    \"\"\"\n",
    "    Completely or partially transform images to their superpixel representation.\n",
    "\n",
    "    Args:\n",
    "        p_replace (int) : Defines the probability of any superpixel area being\n",
    "        replaced by the superpixel.\n",
    "\n",
    "        n_segments (int): Target number of superpixels to generate.\n",
    "        Lower numbers are faster.\n",
    "\n",
    "        interpolation (str): Interpolation to use. Can be one of 'nearest',\n",
    "        'bilinear' defaults to nearest\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p_replace=0, n_segments=0, max_size=360,\n",
    "                 interpolation=\"bilinear\"):\n",
    "        self.p_replace = p_replace\n",
    "        self.n_segments = n_segments\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        is_PIL = isinstance(clip[0], PIL.Image.Image)\n",
    "        if is_PIL:\n",
    "            clip = [np.asarray(img) for img in clip]\n",
    "\n",
    "        # TODO this results in an error when n_segments is 0\n",
    "        replace_samples = np.tile(np.array([self.p_replace]), self.n_segments)\n",
    "        avg_image = np.mean(clip, axis=0)\n",
    "        segments = segmentation.slic(avg_image, n_segments=self.n_segments,\n",
    "                                     compactness=10)\n",
    "\n",
    "        if not np.max(replace_samples) == 0:\n",
    "            print(\"Converting\")\n",
    "            clip = [self._apply_segmentation(img, replace_samples, segments) for img in clip]\n",
    "\n",
    "        if is_PIL:\n",
    "            return [PIL.Image.fromarray(img) for img in clip]\n",
    "        else:\n",
    "            return clip\n",
    "\n",
    "    def _apply_segmentation(self, image, replace_samples, segments):\n",
    "        nb_channels = image.shape[2]\n",
    "        image_sp = np.copy(image)\n",
    "        for c in range(nb_channels):\n",
    "            # segments+1 here because otherwise regionprops always misses\n",
    "            # the last label\n",
    "            regions = measure.regionprops(segments + 1,\n",
    "                                          intensity_image=image[..., c])\n",
    "            for ridx, region in enumerate(regions):\n",
    "                # with mod here, because slic can sometimes create more \n",
    "                # superpixel than requested. replace_samples then does \n",
    "                # not have enough values, so we just start over with the\n",
    "                # first one again.\n",
    "                if replace_samples[ridx % len(replace_samples)] == 1:\n",
    "                    mean_intensity = region.mean_intensity\n",
    "                    image_sp_c = image_sp[..., c]\n",
    "                    image_sp_c[segments == ridx] = mean_intensity\n",
    "\n",
    "        return image_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4eb50548",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Augmenters that apply to a group of augmentations, like selecting\n",
    "an augmentation from a list, or applying all the augmentations in\n",
    "a list sequentially\n",
    "\n",
    "To use the augmenters, clone the complete repo and use\n",
    "`from vidaug import augmenters as va`\n",
    "and then e.g. :\n",
    "    seq = va.Sequential([ va.HorizontalFlip(),\n",
    "                          va.VerticalFlip() ])\n",
    "\n",
    "List of augmenters:\n",
    "    * Sequential\n",
    "    * OneOf\n",
    "    * SomeOf\n",
    "    * Sometimes\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import random\n",
    "\n",
    "\n",
    "class Sequential(object):\n",
    "    \"\"\"\n",
    "    Composes several augmentations together.\n",
    "\n",
    "    Args:\n",
    "        transforms (list of \"Augmentor\" objects): The list of augmentations to compose.\n",
    "\n",
    "        random_order (bool): Whether to apply the augmentations in random order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms, random_order=False):\n",
    "        self.transforms = transforms\n",
    "        self.rand = random_order\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        if self.rand:\n",
    "            rand_transforms = self.transforms[:]\n",
    "            random.shuffle(rand_transforms)\n",
    "            for t in rand_transforms:\n",
    "                clip = t(clip)\n",
    "        else:\n",
    "            for t in self.transforms:\n",
    "                clip = t(clip)\n",
    "\n",
    "        return clip\n",
    "\n",
    "\n",
    "class OneOf(object):\n",
    "    \"\"\"\n",
    "    Selects one augmentation from a list.\n",
    "\n",
    "    Args:\n",
    "        transforms (list of \"Augmentor\" objects): The list of augmentations to compose.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        select = random.choice(self.transforms)\n",
    "        clip = select(clip)\n",
    "        return clip\n",
    "\n",
    "\n",
    "class SomeOf(object):\n",
    "    \"\"\"\n",
    "    Selects a given number of augmentation from a list.\n",
    "\n",
    "    Args:\n",
    "        transforms (list of \"Augmentor\" objects): The list of augmentations.\n",
    "\n",
    "        N (int): The number of augmentations to select from the list.\n",
    "\n",
    "        random_order (bool): Whether to apply the augmentations in random order.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms, N, random_order=True):\n",
    "        self.transforms = transforms\n",
    "        self.rand = random_order\n",
    "        if N > len(transforms):\n",
    "            raise TypeError('The number of applied augmentors should be smaller than the given augmentation number')\n",
    "        else:\n",
    "            self.N = N\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        if self.rand:\n",
    "            tmp = self.transforms[:]\n",
    "            selected_trans = [tmp.pop(random.randrange(len(tmp))) for _ in range(self.N)]\n",
    "            for t in selected_trans:\n",
    "                clip = t(clip)\n",
    "            return clip\n",
    "        else:\n",
    "            indices = [i for i in range(len(self.transforms))]\n",
    "            selected_indices = [indices.pop(random.randrange(len(indices)))\n",
    "                                for _ in range(self.N)]\n",
    "            selected_indices.sort()\n",
    "            selected_trans = [self.transforms[i] for i in selected_indices]\n",
    "            for t in selected_trans:\n",
    "                clip = t(clip)\n",
    "            return clip\n",
    "\n",
    "\n",
    "class Sometimes(object):\n",
    "    \"\"\"\n",
    "    Applies an augmentation with a given probability.\n",
    "\n",
    "    Args:\n",
    "        p (float): The probability to apply the augmentation.\n",
    "\n",
    "        transform (an \"Augmentor\" object): The augmentation to apply.\n",
    "\n",
    "    Example: Use this this transform as follows:\n",
    "        sometimes = lambda aug: va.Sometimes(0.5, aug)\n",
    "        sometimes(va.HorizontalFlip)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p, transform):\n",
    "        self.transform = transform\n",
    "        if (p > 1.0) | (p < 0.0):\n",
    "            raise TypeError('Expected p to be in [0.0 <= 1.0], ' +\n",
    "                            'but got p = {0}'.format(p))\n",
    "        else:\n",
    "            self.p = p\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        if random.random() < self.p:\n",
    "            clip = self.transform(clip)\n",
    "        return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33a4a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Augmenters that apply transformations on the pixel intensities.\n",
    "\n",
    "To use the augmenters, clone the complete repo and use\n",
    "`from vidaug import augmenters as va`\n",
    "and then e.g. :\n",
    "    seq = va.Sequential([ va.RandomRotate(30),\n",
    "                          va.RandomResize(0.2)  ])\n",
    "\n",
    "List of augmenters:\n",
    "    * InvertColor\n",
    "    * Add\n",
    "    * Multiply\n",
    "    * Pepper\n",
    "    * Salt\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import PIL\n",
    "from PIL import ImageOps\n",
    "\n",
    "\n",
    "\n",
    "class InvertColor(object):\n",
    "    \"\"\"\n",
    "    Inverts the color of the video.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            return [np.invert(img) for img in clip]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            inverted = [ImageOps.invert(img) for img in clip]\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "\n",
    "        return inverted\n",
    "\n",
    "\n",
    "class Add(object):\n",
    "    \"\"\"\n",
    "    Add a value to all pixel intesities in an video.\n",
    "\n",
    "    Args:\n",
    "        value (int): The value to be added to pixel intesities.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value=0):\n",
    "        if value > 255 or value < -255:\n",
    "            raise TypeError('The video is blacked or whitened out since ' +\n",
    "                            'value > 255 or value < -255.')\n",
    "        self.value = value\n",
    "\n",
    "    def __call__(self, clip):\n",
    "\n",
    "        is_PIL = isinstance(clip[0], PIL.Image.Image)\n",
    "        if is_PIL:\n",
    "            clip = [np.asarray(img) for img in clip]\n",
    "\n",
    "        data_final = []\n",
    "        for i in range(len(clip)):\n",
    "            image = clip[i].astype(np.int32)\n",
    "            image += self.value\n",
    "            image = np.where(image > 255, 255, image)\n",
    "            image = np.where(image < 0, 0, image)\n",
    "            image = image.astype(np.uint8)\n",
    "            data_final.append(image.astype(np.uint8))\n",
    "\n",
    "        if is_PIL:\n",
    "            return [PIL.Image.fromarray(img) for img in data_final]\n",
    "        else:\n",
    "            return data_final\n",
    "\n",
    "\n",
    "class Multiply(object):\n",
    "    \"\"\"\n",
    "    Multiply all pixel intensities with given value.\n",
    "    This augmenter can be used to make images lighter or darker.\n",
    "\n",
    "    Args:\n",
    "        value (float): The value with which to multiply the pixel intensities\n",
    "        of video.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value=1.0):\n",
    "        if value < 0.0:\n",
    "            raise TypeError('The video is blacked out since for value < 0.0')\n",
    "        self.value = value\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        is_PIL = isinstance(clip[0], PIL.Image.Image)\n",
    "        if is_PIL:\n",
    "            clip = [np.asarray(img) for img in clip]\n",
    "\n",
    "        data_final = []\n",
    "        for i in range(len(clip)):\n",
    "            image = clip[i].astype(np.float64)\n",
    "            image *= self.value\n",
    "            image = np.where(image > 255, 255, image)\n",
    "            image = np.where(image < 0, 0, image)\n",
    "            image = image.astype(np.uint8)\n",
    "            data_final.append(image.astype(np.uint8))\n",
    "\n",
    "        if is_PIL:\n",
    "            return [PIL.Image.fromarray(img) for img in data_final]\n",
    "        else:\n",
    "            return data_final\n",
    "\n",
    "\n",
    "class Pepper(object):\n",
    "    \"\"\"\n",
    "    Augmenter that sets a certain fraction of pixel intensities to 0, hence\n",
    "    they become black.\n",
    "\n",
    "    Args:\n",
    "        ratio (int): Determines number of black pixels on each frame of video.\n",
    "        Smaller the ratio, higher the number of black pixels.\n",
    "    \"\"\"\n",
    "    def __init__(self, ratio=100):\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        is_PIL = isinstance(clip[0], PIL.Image.Image)\n",
    "        if is_PIL:\n",
    "            clip = [np.asarray(img) for img in clip]\n",
    "\n",
    "        data_final = []\n",
    "        for i in range(len(clip)):\n",
    "            img = clip[i].astype(float)\n",
    "            img_shape = img.shape\n",
    "            noise = np.random.randint(self.ratio, size=img_shape)\n",
    "            img = np.where(noise == 0, 0, img)\n",
    "            data_final.append(img.astype(np.uint8))\n",
    "\n",
    "        if is_PIL:\n",
    "            return [PIL.Image.fromarray(img) for img in data_final]\n",
    "        else:\n",
    "            return data_final\n",
    "\n",
    "class Salt(object):\n",
    "    \"\"\"\n",
    "    Augmenter that sets a certain fraction of pixel intesities to 255, hence\n",
    "    they become white.\n",
    "\n",
    "    Args:\n",
    "        ratio (int): Determines number of white pixels on each frame of video.\n",
    "        Smaller the ratio, higher the number of white pixels.\n",
    "   \"\"\"\n",
    "    def __init__(self, ratio=100):\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        is_PIL = isinstance(clip[0], PIL.Image.Image)\n",
    "        if is_PIL:\n",
    "            clip = [np.asarray(img) for img in clip]\n",
    "\n",
    "        data_final = []\n",
    "        for i in range(len(clip)):\n",
    "            img = clip[i].astype(float)\n",
    "            img_shape = img.shape\n",
    "            noise = np.random.randint(self.ratio, size=img_shape)\n",
    "            img = np.where(noise == 0, 255, img)\n",
    "            data_final.append(img.astype(np.uint8))\n",
    "\n",
    "        if is_PIL:\n",
    "            return [PIL.Image.fromarray(img) for img in data_final]\n",
    "        else:\n",
    "            return data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26ca9270",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Augmenters that apply temporal transformations.\n",
    "\n",
    "To use the augmenters, clone the complete repo and use\n",
    "`from vidaug import augmenters as va`\n",
    "and then e.g. :\n",
    "    seq = va.Sequential([ va.RandomRotate(30),\n",
    "                          va.RandomResize(0.2)  ])\n",
    "\n",
    "List of augmenters:\n",
    "    * TemporalBeginCrop\n",
    "    * TemporalCenterCrop\n",
    "    * TemporalRandomCrop\n",
    "    * InverseOrder\n",
    "    * Downsample\n",
    "    * Upsample\n",
    "    * TemporalFit\n",
    "    * TemporalElasticTransformation\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "class TemporalBeginCrop(object):\n",
    "    \"\"\"\n",
    "    Temporally crop the given frame indices at a beginning.\n",
    "    If the number of frames is less than the size,\n",
    "    loop the indices as many times as necessary to satisfy the size.\n",
    "\n",
    "    Args:\n",
    "        size (int): Desired output size of the crop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        out = clip[:self.size]\n",
    "\n",
    "        for img in out:\n",
    "            if len(out) >= self.size:\n",
    "                break\n",
    "            out.append(img)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class TemporalCenterCrop(object):\n",
    "    \"\"\"\n",
    "    Temporally crop the given frame indices at a center.\n",
    "    If the number of frames is less than the size,\n",
    "    loop the indices as many times as necessary to satisfy the size.\n",
    "\n",
    "    Args:\n",
    "        size (int): Desired output size of the crop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        center_index = len(clip) // 2\n",
    "        begin_index = max(0, center_index - (self.size // 2))\n",
    "        end_index = min(begin_index + self.size, len(clip))\n",
    "\n",
    "        out = clip[begin_index:end_index]\n",
    "\n",
    "        for img in out:\n",
    "            if len(out) >= self.size:\n",
    "                break\n",
    "            out.append(img)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class TemporalRandomCrop(object):\n",
    "    \"\"\"\n",
    "    Temporally crop the given frame indices at a random location.\n",
    "    If the number of frames is less than the size,\n",
    "    loop the indices as many times as necessary to satisfy the size.\n",
    "\n",
    "    Args:\n",
    "        size (int): Desired output size of the crop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        rand_end = max(0, len(clip) - self.size - 1)\n",
    "        begin_index = random.randint(0, rand_end)\n",
    "        end_index = min(begin_index + self.size, len(clip))\n",
    "\n",
    "        out = clip[begin_index:end_index]\n",
    "\n",
    "        for img in out:\n",
    "            if len(out) >= self.size:\n",
    "                break\n",
    "            out.append(img)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class InverseOrder(object):\n",
    "    \"\"\"\n",
    "    Inverts the order of clip frames.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        for i in range(len(clip)):\n",
    "            nb_images = len(clip)\n",
    "            return [clip[img] for img in reversed(range(1, nb_images))]\n",
    "\n",
    "\n",
    "class Downsample(object):\n",
    "    \"\"\"\n",
    "    Temporally downsample a video by deleting some of its frames.\n",
    "\n",
    "    Args:\n",
    "        ratio (float): Downsampling ratio in [0.0 <= ratio <= 1.0].\n",
    "    \"\"\"\n",
    "    def __init__(self , ratio=1.0):\n",
    "        if ratio < 0.0 or ratio > 1.0:\n",
    "            raise TypeError('ratio should be in [0.0 <= ratio <= 1.0]. ' +\n",
    "                            'Please use upsampling for ratio > 1.0')\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        nb_return_frame = np.floor(self.ratio * len(clip)).astype(int)\n",
    "        return_ind = [int(i) for i in np.linspace(1, len(clip), num=nb_return_frame)]\n",
    "\n",
    "        return [clip[i-1] for i in return_ind]\n",
    "\n",
    "\n",
    "class Upsample(object):\n",
    "    \"\"\"\n",
    "    Temporally upsampling a video by deleting some of its frames.\n",
    "\n",
    "    Args:\n",
    "        ratio (float): Upsampling ratio in [1.0 < ratio < infinity].\n",
    "    \"\"\"\n",
    "    def __init__(self , ratio=1.0):\n",
    "        if ratio < 1.0:\n",
    "            raise TypeError('ratio should be 1.0 < ratio. ' +\n",
    "                            'Please use downsampling for ratio <= 1.0')\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        nb_return_frame = np.floor(self.ratio * len(clip)).astype(int)\n",
    "        return_ind = [int(i) for i in np.linspace(1, len(clip), num=nb_return_frame)]\n",
    "\n",
    "        return [clip[i-1] for i in return_ind]\n",
    "\n",
    "\n",
    "class TemporalFit(object):\n",
    "    \"\"\"\n",
    "    Temporally fits a video to a given frame size by\n",
    "    downsampling or upsampling.\n",
    "\n",
    "    Args:\n",
    "        size (int): Frame size to fit the video.\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        if size < 0:\n",
    "            raise TypeError('size should be positive')\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        return_ind = [int(i) for i in np.linspace(1, len(clip), num=self.size)]\n",
    "\n",
    "        return [clip[i-1] for i in return_ind]\n",
    "\n",
    "\n",
    "class TemporalElasticTransformation(object):\n",
    "    \"\"\"\n",
    "    Stretches or schrinks a video at the beginning, end or middle parts.\n",
    "    In normal operation, augmenter stretches the beggining and end, schrinks\n",
    "    the center.\n",
    "    In inverse operation, augmenter shrinks the beggining and end, stretches\n",
    "    the center.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        nb_images = len(clip)\n",
    "        new_indices = self._get_distorted_indices(nb_images)\n",
    "        return [clip[i] for i in new_indices]\n",
    "\n",
    "    def _get_distorted_indices(self, nb_images):\n",
    "        inverse = random.randint(0, 1)\n",
    "\n",
    "        if inverse:\n",
    "            scale = random.random()\n",
    "            scale *= 0.21\n",
    "            scale += 0.6\n",
    "        else:\n",
    "            scale = random.random()\n",
    "            scale *= 0.6\n",
    "            scale += 0.8\n",
    "\n",
    "        frames_per_clip = nb_images\n",
    "\n",
    "        indices = np.linspace(-scale, scale, frames_per_clip).tolist()\n",
    "        if inverse:\n",
    "            values = [math.atanh(x) for x in indices]\n",
    "        else:\n",
    "            values = [math.tanh(x) for x in indices]\n",
    "\n",
    "        values = [x / values[-1] for x in values]\n",
    "        values = [int(round(((x + 1) / 2) * (frames_per_clip - 1), 0)) for x in values]\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dde1cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "609adcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'C:\\\\Users\\\\sarat\\\\Desktop\\\\videos'\n",
    "output_dir = 'C:\\\\Users\\\\sarat\\\\Desktop\\\\video_aug'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40cdf71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9fbc6573",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 24883200 bytes in function 'cv::OutOfMemoryError'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m seq \u001b[38;5;241m=\u001b[39m Sequential([sometimes(RandomShear(\u001b[38;5;241m0.45\u001b[39m,\u001b[38;5;241m0.30\u001b[39m)),sometimes(Salt()),sometimes(Pepper())])\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#augment the frames\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m video_aug \u001b[38;5;241m=\u001b[39m seq(frames)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# output the video\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m video_aug:\n",
      "Cell \u001b[1;32mIn[36], line 47\u001b[0m, in \u001b[0;36mSequential.__call__\u001b[1;34m(self, clip)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 47\u001b[0m         clip \u001b[38;5;241m=\u001b[39m t(clip)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clip\n",
      "Cell \u001b[1;32mIn[36], line 132\u001b[0m, in \u001b[0;36mSometimes.__call__\u001b[1;34m(self, clip)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, clip):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp:\n\u001b[1;32m--> 132\u001b[0m         clip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(clip)\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clip\n",
      "Cell \u001b[1;32mIn[32], line 166\u001b[0m, in \u001b[0;36mRandomShear.__call__\u001b[1;34m(self, clip)\u001b[0m\n\u001b[0;32m    164\u001b[0m     rows, cols, ch \u001b[38;5;241m=\u001b[39m clip[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    165\u001b[0m     transform_mat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32([[\u001b[38;5;241m1\u001b[39m, x_shear, \u001b[38;5;241m0\u001b[39m], [y_shear, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [cv2\u001b[38;5;241m.\u001b[39mwarpAffine(img, transform_mat, (cols, rows)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m clip]\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(clip[\u001b[38;5;241m0\u001b[39m], PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [img\u001b[38;5;241m.\u001b[39mtransform(img\u001b[38;5;241m.\u001b[39msize, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mAFFINE, (\u001b[38;5;241m1\u001b[39m, x_shear, \u001b[38;5;241m0\u001b[39m, y_shear, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m clip]\n",
      "Cell \u001b[1;32mIn[32], line 166\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    164\u001b[0m     rows, cols, ch \u001b[38;5;241m=\u001b[39m clip[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    165\u001b[0m     transform_mat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32([[\u001b[38;5;241m1\u001b[39m, x_shear, \u001b[38;5;241m0\u001b[39m], [y_shear, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [cv2\u001b[38;5;241m.\u001b[39mwarpAffine(img, transform_mat, (cols, rows)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m clip]\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(clip[\u001b[38;5;241m0\u001b[39m], PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [img\u001b[38;5;241m.\u001b[39mtransform(img\u001b[38;5;241m.\u001b[39msize, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mAFFINE, (\u001b[38;5;241m1\u001b[39m, x_shear, \u001b[38;5;241m0\u001b[39m, y_shear, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m clip]\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 24883200 bytes in function 'cv::OutOfMemoryError'\n"
     ]
    }
   ],
   "source": [
    "# Loop over each video file in the input directory\n",
    "for file in os.listdir(os.path.join(input_dir,action)):\n",
    "    video_path = os.path.join(input_dir,action, file)\n",
    "    \n",
    "    # Load the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Define the output video writer\n",
    "    # Meta.\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_size = (width, height)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_path = os.path.join(output_dir,file)\n",
    "    # Video writer.\n",
    "    out = cv2.VideoWriter(out_path, fourcc, fps, frame_size)\n",
    "\n",
    "    # collect all frames of the video\n",
    "    frames = []\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "\n",
    "    # Apply the video augmentation pipeline to each frame of the video\n",
    "    sometimes = lambda aug: Sometimes(0.5, aug) # Used to apply augmentor with 50% probability\n",
    "    seq = Sequential([sometimes(RandomShear(0.45,0.30)),sometimes(Salt()),sometimes(Pepper())])\n",
    "    #augment the frames\n",
    "    video_aug = seq(frames)\n",
    "    \n",
    "    # output the video\n",
    "    for frame in video_aug:\n",
    "        out.write(frame)\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91c9341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
